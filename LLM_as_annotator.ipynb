{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1cBUTeReznuoWktrzzyuwMboLFhkHfbay",
      "authorship_tag": "ABX9TyPiW1ZzeiDf7HdxKkCbFOGY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dylan5lin/MP_AnnotatorDisagreement/blob/main/LLM_as_annotator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using LLM's as annotators using MultiPiCo dataset from LeWiDi 2025.\n",
        "Goal of this experiment is to find out whether our selected Large Language Model, DeepSeek-V3, is capable of capturing annotator disagreement. First, we use Zero-Shot prompting. Afterwards, Few-Shot prompting will be applied as well to investigate its learning ability. The dataset that will be used will come from the third edition of the Learning With Disagreements shared task as part of SemEval. The MultiPiCo dataset contains annotated data on an irony detection task. DeepSeek-V3 will be tasked to provide a soft label distribution, with binary classification (1=ironic, 0=non-ironic)."
      ],
      "metadata": {
        "id": "0tQvsNDfutBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required installations"
      ],
      "metadata": {
        "id": "WjWSdZ2zwzZM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A0a3dIgZw3R9",
        "outputId": "8aaf00b1-48ab-44fa-c1dd-51ce4f718e96"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.78.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.4)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading the dataset"
      ],
      "metadata": {
        "id": "YKhnXA2IwVjl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SRWDJE1eppk5",
        "outputId": "1e27a310-ea53-456c-d895-0f1ad90d93c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/drive/My Drive/data_practice_phase/data_practice_phase/MP/MP_train.json'\n",
        "with open(file_path, 'r') as file:\n",
        "  data = json.load(file)"
      ],
      "metadata": {
        "id": "YWR5hJF4sICX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(data)\n",
        "df = df.transpose()\n",
        "# For this experiment, we will use Dutch annotation tasks only.\n",
        "dutch_df = df[df['lang'] == 'nl']\n",
        "print(dutch_df.head())\n",
        "print(len(dutch_df))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VFjyOJansKm-",
        "outputId": "f9ce8c4e-a0fe-476f-dc8e-ba8fdb5145a5"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      annotation task                                               text  \\\n",
            "8134  irony detection  {'post': 'De dood is minder dan het geraamte, ...   \n",
            "8135  irony detection  {'post': 'Die vraag of leeftijd was wel een be...   \n",
            "8143  irony detection  {'post': '@USER Gefeliciteerd!', 'reply': '@US...   \n",
            "8144  irony detection  {'post': 'Gezamenlijke maaltijden zijn ontzett...   \n",
            "8146  irony detection  {'post': 'Tegenwoordig hebben veel mensen een ...   \n",
            "\n",
            "                              annotators number of annotations  \\\n",
            "8134  Ann215,Ann216,Ann226,Ann232,Ann236                     5   \n",
            "8135  Ann215,Ann220,Ann225,Ann230,Ann237                     5   \n",
            "8143  Ann215,Ann216,Ann225,Ann233,Ann235                     5   \n",
            "8144  Ann215,Ann218,Ann219,Ann223,Ann228                     5   \n",
            "8146  Ann215,Ann218,Ann224,Ann230,Ann231                     5   \n",
            "\n",
            "                                            annotations  \\\n",
            "8134  {'Ann215': '0', 'Ann216': '0', 'Ann226': '0', ...   \n",
            "8135  {'Ann215': '0', 'Ann220': '0', 'Ann225': '0', ...   \n",
            "8143  {'Ann215': '0', 'Ann216': '0', 'Ann225': '0', ...   \n",
            "8144  {'Ann215': '1', 'Ann218': '1', 'Ann219': '0', ...   \n",
            "8146  {'Ann215': '0', 'Ann218': '0', 'Ann224': '0', ...   \n",
            "\n",
            "                    soft_label  split lang  \\\n",
            "8134    {'0.0': 1.0, '1.0': 0}  train   nl   \n",
            "8135    {'0.0': 1.0, '1.0': 0}  train   nl   \n",
            "8143    {'0.0': 1.0, '1.0': 0}  train   nl   \n",
            "8144  {'0.0': 0.4, '1.0': 0.6}  train   nl   \n",
            "8146    {'0.0': 1.0, '1.0': 0}  train   nl   \n",
            "\n",
            "                                             other_info  \n",
            "8134  {'source': 'twitter', 'level': 1.0, 'language_...  \n",
            "8135  {'source': 'reddit', 'level': 1.0, 'language_v...  \n",
            "8143  {'source': 'twitter', 'level': 2.0, 'language_...  \n",
            "8144  {'source': 'twitter', 'level': 1.0, 'language_...  \n",
            "8146  {'source': 'reddit', 'level': 1.0, 'language_v...  \n",
            "637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading DeepSeek-V3"
      ],
      "metadata": {
        "id": "05pFhVGGuqv8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=\"sk-cc8ca3c49efc4c5782bec2087763e4fa\", base_url=\"https://api.deepseek.com\")\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model=\"deepseek-chat\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
        "        {\"role\": \"user\", \"content\": \"Hi!\"},\n",
        "    ],\n",
        "    stream=False\n",
        ")\n",
        "print(response.choices[0].message.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5uqPXXGUuqY_",
        "outputId": "f238b9e2-9395-4c85-a361-6a9f36155d53"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! ðŸ˜Š How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prompting techniques"
      ],
      "metadata": {
        "id": "UO9pDHz5y_SW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We start off with Zero-Shot prompting. We feed the LLM the message and ask it to provide a soft label distribution."
      ],
      "metadata": {
        "id": "Dn-KY-QszE31"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zero_shot_prompt(text, system_prompt):\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following post-reply message whether it is ironic.\n",
        "    Provide a soft label distribution for whether the reply to the post is ironic (1) or non-ironic (0).\n",
        "    Text: \"{text}\"\n",
        "    Requirements for the Output format:\n",
        "    A JSON object with keys '0'=non_ironic and '1'=ironic, values must be between 0 and 1.\n",
        "    The values must sum up to 1.\n",
        "    Example Output:\n",
        "    {{'0':0.6, '1':0.4}}\n",
        "\n",
        "    Analyze the provided text and return the output as mentioned above.\n",
        "    \"\"\"\n",
        "    response = client.chat.completions.create(\n",
        "        model=\"deepseek-chat\",\n",
        "        messages=[{\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": prompt}],\n",
        "        response_format={\"type\": \"json_object\"},\n",
        "        stream=False\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "print(zero_shot_prompt(dutch_df['text'][0], \"You are an assistant that annotates post-reply messages on irony\"), dutch_df['text'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lwxnI8qt1drV",
        "outputId": "d8f5a50e-4af8-4875-bd30-6fc841b5b76b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-21-a6c839e2d5e5>:23: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
            "  print(zero_shot_prompt(dutch_df['text'][0], \"You are an assistant that annotates post-reply messages on irony\"), dutch_df['text'][0])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\"0\":0.8, \"1\":0.2} {'post': 'De dood is minder dan het geraamte, het levenâ€¦..', 'reply': '@USER Het is de jas die het geraamte omhuld en het karakter wat levenslust naar buiten brengt.'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we use Few-Shot prompting. We feed the LLM three examples, and then the message. We ask it to provide a soft-label distribution."
      ],
      "metadata": {
        "id": "pXMxFnzn7dIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def few_shot_prompt(system_prompt, few_shot_examples, user_prompt):\n",
        "  prompt = f\"\"\"\n",
        "  Analyze the following post-reply message whether it is ironic.\n",
        "  Provide a soft label distribution for whether the reply to the post is ironic (1) or non-ironic (0).\n",
        "  A JSON object with keys '0'=non_ironic and '1'=ironic, values must be between 0 and 1.\n",
        "    The values must sum up to 1.\n",
        "    Example Output:\n",
        "    {{'0':0.6, '1':0.4}}\n",
        "    Analyze the provided text and return the output as mentioned above.\n",
        "    \"\"\"\n",
        "  messages = []\n",
        "  messages.append({\"role\": \"system\", \"content\": system_prompt})\n",
        "  for user_message, assistant_message in few_shot_examples:\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt + '\\n' + user_message})\n",
        "    messages.append({\"role\": \"assistant\", \"content\": assistant_message})\n",
        "  messages.append({\"role\": \"user\", \"content\": prompt + '\\n' + user_prompt})\n",
        "  response = client.chat.completions.create(\n",
        "    model=\"deepseek-chat\",\n",
        "    messages=messages,\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    stream=False\n",
        "  )\n",
        "  return response.choices[0].message.content\n",
        "\n"
      ],
      "metadata": {
        "id": "Fzhq0OIf6Kk3"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run dataset on LLM using Zero-shot and Few-shot prompting"
      ],
      "metadata": {
        "id": "_HT1vwVRqvmh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_prompt_zeroshot(df):\n",
        "  prompts = df['text'].tolist()\n",
        "  results = df[['text', 'soft_label']]\n",
        "  results['soft_label_prediction'] = ''\n",
        "  system_prompt = \"You are an assistant that annotates post-reply messages on irony\"\n",
        "  for prompt in prompts:\n",
        "    results.loc[results['text'] == prompt, 'soft_label_prediction'] = zero_shot_prompt(prompt, system_prompt)\n",
        "  return results\n",
        "zeroshot_res = run_prompt_zeroshot(dutch_df[1:10])\n",
        "def run_prompt_fewshot(df):\n",
        "  prompts = df['text'].tolist()\n",
        "  results = df[['text', 'soft_label']]\n",
        "  results['soft_label_prediction'] = ''\n",
        "  system_prompt = \"You are an assistant that annotates post-reply messages on irony\"\n",
        "  few_shot_examples = []\n",
        "  for prompt in prompts:\n",
        "    results.loc[results['text'] == prompt, 'soft_label_prediction'] = few_shot_prompt(system_prompt, few_shot_examples, prompt)\n",
        "  return"
      ],
      "metadata": {
        "id": "sBGwY8ECqvLJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ada04b4-4ded-4523-e418-a4e14f142534"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-48-f1a96f89f480>:4: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  results['soft_label_prediction'] = ''\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for item in zeroshot_res['soft_label']:\n",
        "  try:\n",
        "    item['non_ironic'] = item.pop('0.0')\n",
        "    item['ironic'] = item.pop('1.0')\n",
        "  except KeyError:\n",
        "    break\n",
        "predictions = []\n",
        "for item in zeroshot_res['soft_label_prediction']:\n",
        "  pred_ = json.loads(item)\n",
        "  try:\n",
        "    pred_['non_ironic'] = pred_.pop(\"0\")\n",
        "    pred_['ironic'] = pred_.pop(\"1\")\n",
        "    predictions.append(pred_)\n",
        "  except KeyError:\n",
        "    break\n",
        "zeroshot_res['soft_label_prediction'] = predictions\n",
        "print(zeroshot_res)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MfCxp273MpRB",
        "outputId": "54446811-2b26-4517-e03e-eba4609e1f10"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                   text  \\\n",
            "8135  {'post': 'Die vraag of leeftijd was wel een be...   \n",
            "8143  {'post': '@USER Gefeliciteerd!', 'reply': '@US...   \n",
            "8144  {'post': 'Gezamenlijke maaltijden zijn ontzett...   \n",
            "8146  {'post': 'Tegenwoordig hebben veel mensen een ...   \n",
            "8147  {'post': 'Het is een puppet van Schwab. Het en...   \n",
            "8148  {'post': 'Raheem Sterling is replaced by Pierr...   \n",
            "8150  {'post': 'Ik ken het Dyslexie lettertype waar ...   \n",
            "8151  {'post': 'Gedaan.  Als partner van een enorme ...   \n",
            "8152  {'post': 'rellen =/= demonstreren. Als je je a...   \n",
            "\n",
            "                              soft_label soft_label_prediction  \\\n",
            "8135    {'non_ironic': 1.0, 'ironic': 0}    {\"0\":0.9, \"1\":0.1}   \n",
            "8143    {'non_ironic': 1.0, 'ironic': 0}    {\"0\":0.9, \"1\":0.1}   \n",
            "8144  {'non_ironic': 0.4, 'ironic': 0.6}    {\"0\":0.3, \"1\":0.7}   \n",
            "8146    {'non_ironic': 1.0, 'ironic': 0}    {\"0\":0.3, \"1\":0.7}   \n",
            "8147  {'non_ironic': 0.2, 'ironic': 0.8}     {\"0\":0.3,\"1\":0.7}   \n",
            "8148  {'non_ironic': 0.4, 'ironic': 0.6}    {\"0\":0.3, \"1\":0.7}   \n",
            "8150    {'non_ironic': 1.0, 'ironic': 0}    {\"0\":0.8, \"1\":0.2}   \n",
            "8151    {'non_ironic': 1.0, 'ironic': 0}    {\"0\":0.3, \"1\":0.7}   \n",
            "8152  {'non_ironic': 0.6, 'ironic': 0.4}    {\"0\":0.3, \"1\":0.7}   \n",
            "\n",
            "     softlabel_prediction  \n",
            "8135                  NaN  \n",
            "8143                  NaN  \n",
            "8144                  NaN  \n",
            "8146                  NaN  \n",
            "8147                  NaN  \n",
            "8148                  NaN  \n",
            "8150                  NaN  \n",
            "8151                  NaN  \n",
            "8152                  NaN  \n",
            "                                                   text  \\\n",
            "8135  {'post': 'Die vraag of leeftijd was wel een be...   \n",
            "8143  {'post': '@USER Gefeliciteerd!', 'reply': '@US...   \n",
            "8144  {'post': 'Gezamenlijke maaltijden zijn ontzett...   \n",
            "8146  {'post': 'Tegenwoordig hebben veel mensen een ...   \n",
            "8147  {'post': 'Het is een puppet van Schwab. Het en...   \n",
            "8148  {'post': 'Raheem Sterling is replaced by Pierr...   \n",
            "8150  {'post': 'Ik ken het Dyslexie lettertype waar ...   \n",
            "8151  {'post': 'Gedaan.  Als partner van een enorme ...   \n",
            "8152  {'post': 'rellen =/= demonstreren. Als je je a...   \n",
            "\n",
            "                              soft_label               soft_label_prediction  \\\n",
            "8135    {'non_ironic': 1.0, 'ironic': 0}  {'non_ironic': 0.9, 'ironic': 0.1}   \n",
            "8143    {'non_ironic': 1.0, 'ironic': 0}  {'non_ironic': 0.9, 'ironic': 0.1}   \n",
            "8144  {'non_ironic': 0.4, 'ironic': 0.6}  {'non_ironic': 0.3, 'ironic': 0.7}   \n",
            "8146    {'non_ironic': 1.0, 'ironic': 0}  {'non_ironic': 0.3, 'ironic': 0.7}   \n",
            "8147  {'non_ironic': 0.2, 'ironic': 0.8}  {'non_ironic': 0.3, 'ironic': 0.7}   \n",
            "8148  {'non_ironic': 0.4, 'ironic': 0.6}  {'non_ironic': 0.3, 'ironic': 0.7}   \n",
            "8150    {'non_ironic': 1.0, 'ironic': 0}  {'non_ironic': 0.8, 'ironic': 0.2}   \n",
            "8151    {'non_ironic': 1.0, 'ironic': 0}  {'non_ironic': 0.3, 'ironic': 0.7}   \n",
            "8152  {'non_ironic': 0.6, 'ironic': 0.4}  {'non_ironic': 0.3, 'ironic': 0.7}   \n",
            "\n",
            "     softlabel_prediction  \n",
            "8135                  NaN  \n",
            "8143                  NaN  \n",
            "8144                  NaN  \n",
            "8146                  NaN  \n",
            "8147                  NaN  \n",
            "8148                  NaN  \n",
            "8150                  NaN  \n",
            "8151                  NaN  \n",
            "8152                  NaN  \n"
          ]
        }
      ]
    }
  ]
}